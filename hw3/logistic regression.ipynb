{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.473]\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "class logistic_regression(object):\n",
    "    def __init__(self,train_size,test_size,iteration,eta):\n",
    "        self.train_size=train_size\n",
    "        self.test_size=test_size\n",
    "        self.iteration=iteration\n",
    "        self.eta=eta\n",
    "        \n",
    "    def get_train_data(self):\n",
    "        raw_train=open(\"hw3_train.dat\")\n",
    "        X_train=np.zeros((self.train_size,21))\n",
    "        Y_train=np.zeros((self.train_size,1))\n",
    "        i=0\n",
    "        for line in raw_train:\n",
    "            X_train[i,0]=1\n",
    "            X_train[i,1:21]=line.split(\" \")[1:21]\n",
    "            Y_train[i]=line.split(\" \")[21].strip()\n",
    "            i+=1\n",
    "        return X_train,Y_train\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        raw_test=open(\"hw3_test.dat\")\n",
    "        X_test=np.zeros((self.test_size,21))\n",
    "        Y_test=np.zeros((self.test_size,1))\n",
    "        i=0\n",
    "        for line in raw_test:\n",
    "            X_test[i,0]=1\n",
    "            X_test[i,1:21]=line.split(\" \")[1:21]\n",
    "            Y_test[i]=line.split(\" \")[21].strip()\n",
    "            i+=1\n",
    "        return X_test,Y_test\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def gradient_descent(self,X,Y,sgd=False):       #sgd表示是否用随机梯度下降\n",
    "        w=np.zeros((len(X[0]),1))\n",
    "        if not sgd:\n",
    "            for i in range(self.iteration):\n",
    "                tmp = -Y * (np.dot(X, w))\n",
    "                weight_matrix = self.sigmoid(tmp)\n",
    "                gradient = 1 / (len(X) * 1.0) * (sum(weight_matrix * -Y * X,0).reshape(len(w), 1))\n",
    "                #new_eta=self.eta/np.linalg.norm(gradient)\n",
    "                #w=w-new_eta*gradient\n",
    "                w=w-self.eta*gradient\n",
    "        else:\n",
    "            index=0\n",
    "            for i in range(self.iteration):\n",
    "                if index >= len(Y):\n",
    "                    index=0\n",
    "                tmp = -Y[index,0]*np.dot(w.T,X[index,:].reshape(len(w),1))\n",
    "                w=w-self.eta*self.sigmoid(tmp)*(-Y[index,0]*X[index,:]).reshape(len(w),1)\n",
    "                index+=1\n",
    "        return w\n",
    "    \n",
    "    def test_error_rate(self,X,Y,w):\n",
    "        Y_result=np.sign(np.dot(X,w))\n",
    "        sum_error=sum(Y_result!=Y)\n",
    "        return sum_error/self.test_size\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    c=logistic_regression(1000,3000,2000,0.001)\n",
    "    X_train,Y_train=c.get_train_data()\n",
    "    X_test,Y_test=c.get_test_data()\n",
    "    w=c.gradient_descent(X_train,Y_train,sgd=True)\n",
    "    logistic_error=c.test_error_rate(X_test,Y_test,w)\n",
    "    print(logistic_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#by using sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import train_test_split   #用于需要分成训练集和测试集\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#引入数据\n",
    "def get_train_data(train_size):\n",
    "    raw_train=open(\"hw3_train.dat\")\n",
    "    X_train=np.zeros((train_size,21))\n",
    "    Y_train=np.zeros((train_size,1))\n",
    "    i=0\n",
    "    for line in raw_train:\n",
    "        X_train[i,0]=1\n",
    "        X_train[i,1:21]=line.split(\" \")[1:21]\n",
    "        Y_train[i]=line.split(\" \")[21].strip()\n",
    "        i+=1\n",
    "    return X_train,Y_train\n",
    "\n",
    "def get_test_data(test_size):\n",
    "    raw_test=open(\"hw3_test.dat\")\n",
    "    X_test=np.zeros((test_size,21))\n",
    "    Y_test=np.zeros((test_size,1))\n",
    "    i=0\n",
    "    for line in raw_test:\n",
    "        X_test[i,0]=1\n",
    "        X_test[i,1:21]=line.split(\" \")[1:21]\n",
    "        Y_test[i]=line.split(\" \")[21].strip()\n",
    "        i+=1\n",
    "    return X_test,Y_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train,Y_train=get_train_data(1000)\n",
    "    X_test,Y_test=get_test_data(3000)\n",
    "    lr=LogisticRegression()\n",
    "    lr.fit(X_train,Y_train)\n",
    "    Y_pred=lr.predict(X_test)\n",
    "    w=lr.coef_\n",
    "    logistic_error=np.mean(Y_pred == Y_test)\n",
    "    print(logistic_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
